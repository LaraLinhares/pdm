{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX5-BATCH: More advanced RDD API programming\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17110-5cf4-44f8-b34d-64d7b91cd768",
   "metadata": {},
   "source": [
    "### Download Bike Trip Data (Feb 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede39833-c55e-448d-8c3e-8d0fc1b5e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to s3.amazonaws.com (16.15.193.82:443)\n",
      "wget: can't open 'data/202502-citibike-tripdata.zip': File exists\n"
     ]
    }
   ],
   "source": [
    "!wget -np https://s3.amazonaws.com/tripdata/202502-citibike-tripdata.zip -P data/\n",
    "![ -e \"data/202502-citibike-tripdata_1.csv\" ] || (cd data/ && unzip 202502-citibike-tripdata.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7324ec-415f-4e3d-8bc8-539345a90867",
   "metadata": {},
   "source": [
    "### Data is on three files, let us take a look on one (header + a few lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d225bfd7-52b4-4c44-8268-21a131370fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
      "C1F868EC9F7E49A5,electric_bike,2025-02-06 16:54:02.517,2025-02-06 17:00:48.166,Perry St & Bleecker St,5922.07,Watts St & Greenwich St,5578.02,40.73535398,-74.00483091,40.72405549,-74.00965965,member\n",
      "668DDE0CFA929D5A,electric_bike,2025-02-14 10:09:49.035,2025-02-14 10:21:57.856,Dock 72 Way & Market St,4804.02,Spruce St & Nassau St,5137.10,40.69985,-73.97141,40.71146364,-74.00552427,member\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/202502-citibike-tripdata_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059be046-693f-403a-8b23-8a33b0dca5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/202502-citibike-tripdata.zip\n",
      "replace 202502-citibike-tripdata_3.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip data/202502-citibike-tripdata.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13bca3-6dd6-4215-bebf-8b9f3a6a2dc8",
   "metadata": {},
   "source": [
    "### **Dataset Description**\n",
    "The dataset contains **bike trip records** with the following columns:\n",
    "\n",
    "| Column Name            | Description |\n",
    "|------------------------|-------------|\n",
    "| `ride_id`             | Unique trip identifier |\n",
    "| `rideable_type`       | Type of bike used (e.g., docked, electric) |\n",
    "| `started_at`          | Start timestamp of the trip |\n",
    "| `ended_at`            | End timestamp of the trip |\n",
    "| `start_station_name`  | Name of the start station |\n",
    "| `start_station_id`    | ID of the start station |\n",
    "| `end_station_name`    | Name of the end station |\n",
    "| `end_station_id`      | ID of the end station |\n",
    "| `start_lat`          | Latitude of the start location |\n",
    "| `start_lng`          | Longitude of the start location |\n",
    "| `end_lat`            | Latitude of the end location |\n",
    "| `end_lng`            | Longitude of the end location |\n",
    "| `member_casual`       | User type (`member` for subscribers, `casual` for non-subscribers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213ac02-29c9-4818-b516-70e4bdf3cf43",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess the Data\n",
    "1. Start a **PySpark session (or SparkContext)**.\n",
    "2. Load the dataset as an **RDD**.\n",
    "3. **Remove the header** and filter out malformed rows.\n",
    "4. `#TODO` Do the same for each file. Use [Spark Union transformation function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.union.html) for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4abbe0bc-4bce-4258-b693-fd9e15d45892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext not defined\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 19:44:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['C1F868EC9F7E49A5',\n",
       "  'electric_bike',\n",
       "  '2025-02-06 16:54:02.517',\n",
       "  '2025-02-06 17:00:48.166',\n",
       "  'Perry St & Bleecker St',\n",
       "  '5922.07',\n",
       "  'Watts St & Greenwich St',\n",
       "  '5578.02',\n",
       "  '40.73535398',\n",
       "  '-74.00483091',\n",
       "  '40.72405549',\n",
       "  '-74.00965965',\n",
       "  'member'],\n",
       " ['668DDE0CFA929D5A',\n",
       "  'electric_bike',\n",
       "  '2025-02-14 10:09:49.035',\n",
       "  '2025-02-14 10:21:57.856',\n",
       "  'Dock 72 Way & Market St',\n",
       "  '4804.02',\n",
       "  'Spruce St & Nassau St',\n",
       "  '5137.10',\n",
       "  '40.69985',\n",
       "  '-73.97141',\n",
       "  '40.71146364',\n",
       "  '-74.00552427',\n",
       "  'member']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"local[*]\") # local execution\n",
    "# sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"spark://spark:7077\") # cluster execution\n",
    "\n",
    "# Load data\n",
    "file_path = \"data/202502-citibike-tripdata_1.csv\"\n",
    "raw_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Remove header\n",
    "header = raw_rdd.first()\n",
    "data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Split CSV rows into lists\n",
    "rdd = data_rdd.map(lambda row: row.split(\",\"))\n",
    "\n",
    "# Filter out malformed rows (should have 13 columns)\n",
    "valid_rdd = rdd.filter(lambda cols: len(cols) == 13)\n",
    "\n",
    "valid_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5568fff-57db-4d13-8456-eaf99030d09d",
   "metadata": {},
   "source": [
    "## Creating a single one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7bb34e-ed2f-43d7-8959-a912a8142239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 data/202502-citibike-tripdata_1.csv > data/all_citibike.csv\n",
    "!tail -n +2 -q data/202502-citibike-tripdata_*.csv >> data/all_citibike.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ebfe2c-1bd0-45a5-ae43-e12a1757ddf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['C1F868EC9F7E49A5',\n",
       "  'electric_bike',\n",
       "  '2025-02-06 16:54:02.517',\n",
       "  '2025-02-06 17:00:48.166',\n",
       "  'Perry St & Bleecker St',\n",
       "  '5922.07',\n",
       "  'Watts St & Greenwich St',\n",
       "  '5578.02',\n",
       "  '40.73535398',\n",
       "  '-74.00483091',\n",
       "  '40.72405549',\n",
       "  '-74.00965965',\n",
       "  'member'],\n",
       " ['668DDE0CFA929D5A',\n",
       "  'electric_bike',\n",
       "  '2025-02-14 10:09:49.035',\n",
       "  '2025-02-14 10:21:57.856',\n",
       "  'Dock 72 Way & Market St',\n",
       "  '4804.02',\n",
       "  'Spruce St & Nassau St',\n",
       "  '5137.10',\n",
       "  '40.69985',\n",
       "  '-73.97141',\n",
       "  '40.71146364',\n",
       "  '-74.00552427',\n",
       "  'member']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"local[*]\") # local execution\n",
    "# sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"spark://spark:7077\") # cluster execution\n",
    "\n",
    "# Load data\n",
    "file_path = \"data/all_citibike.csv\"\n",
    "raw_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Remove header\n",
    "header = raw_rdd.first()\n",
    "data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Split CSV rows into lists\n",
    "rdd = data_rdd.map(lambda row: row.split(\",\"))\n",
    "\n",
    "# Filter out malformed rows (should have 13 columns)\n",
    "valid_rdd = rdd.filter(lambda cols: len(cols) == 13)\n",
    "\n",
    "valid_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb4a5d-618a-441a-8eac-848b010fd78e",
   "metadata": {},
   "source": [
    "### Step 2: RDD Partitioning\n",
    "1. Check the **initial number of partitions**.\n",
    "2. Repartition the data for better performance (change the number at will).\n",
    "3. See what happens in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd73fc87-1758-40f2-906f-5c53d77f9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partitions: 12\n"
     ]
    }
   ],
   "source": [
    "# check initial partitions\n",
    "initial_partitions = valid_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# change the number of partitions (this will trigger a full shuffle, to reorganize data)\n",
    "partitioned_rdd = valid_rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4f7863-e3cf-4bcd-b886-c9780b7d23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10) MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   CoalescedRDD[7] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   ShuffledRDD[6] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " +-(12) MapPartitionsRDD[5] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      "    |   PythonRDD[4] at RDD at PythonRDD.scala:53 []\n",
      "    |   data/all_citibike.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |   data/all_citibike.csv HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(partitioned_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f892d3-68dc-4a0d-90df-a9de54ccc69b",
   "metadata": {},
   "source": [
    "### Step 3: Get the top-3 most Popular starting stations\n",
    "1. You should get this information and collect to the drive (tip: function [PySpark RDD sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html), however, it can be more efficient than that by using the [Reduce Action](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html) -- not to be confused with the [ReduceByKey Transformation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html))\n",
    "2. Broadcast this information\n",
    "3. Use the broacast to append to each RDD item a new value: `starting_station_top3`, with values `yes` or `no`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3ce869-e755-4a7b-9b7e-17ffe360051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 stations (current step): [('W 21 St & 6 Ave', 8811), ('Pier 61 at Chelsea Piers', 7761), ('Lafayette St & E 8 St', 7190)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['C1F868EC9F7E49A5',\n",
       "  'electric_bike',\n",
       "  '2025-02-06 16:54:02.517',\n",
       "  '2025-02-06 17:00:48.166',\n",
       "  'Perry St & Bleecker St',\n",
       "  '5922.07',\n",
       "  'Watts St & Greenwich St',\n",
       "  '5578.02',\n",
       "  '40.73535398',\n",
       "  '-74.00483091',\n",
       "  '40.72405549',\n",
       "  '-74.00965965',\n",
       "  'member',\n",
       "  'no'],\n",
       " ['668DDE0CFA929D5A',\n",
       "  'electric_bike',\n",
       "  '2025-02-14 10:09:49.035',\n",
       "  '2025-02-14 10:21:57.856',\n",
       "  'Dock 72 Way & Market St',\n",
       "  '4804.02',\n",
       "  'Spruce St & Nassau St',\n",
       "  '5137.10',\n",
       "  '40.69985',\n",
       "  '-73.97141',\n",
       "  '40.71146364',\n",
       "  '-74.00552427',\n",
       "  'member',\n",
       "  'no'],\n",
       " ['67C09DF051FD00FA',\n",
       "  'classic_bike',\n",
       "  '2025-02-09 17:15:56.170',\n",
       "  '2025-02-09 17:18:56.244',\n",
       "  'E 20 St & 2 Ave',\n",
       "  '5971.08',\n",
       "  'E 14 St & 1 Ave',\n",
       "  '5779.10',\n",
       "  '40.73587678',\n",
       "  '-73.98205027',\n",
       "  '40.73139303364151',\n",
       "  '-73.98286700248718',\n",
       "  'member',\n",
       "  'no'],\n",
       " ['69A6F44485E8B57D',\n",
       "  'electric_bike',\n",
       "  '2025-02-02 11:31:54.899',\n",
       "  '2025-02-02 11:35:10.734',\n",
       "  'E 20 St & 2 Ave',\n",
       "  '5971.08',\n",
       "  'E 14 St & 1 Ave',\n",
       "  '5779.10',\n",
       "  '40.73587678',\n",
       "  '-73.98205027',\n",
       "  '40.73139303364151',\n",
       "  '-73.98286700248718',\n",
       "  'member',\n",
       "  'no'],\n",
       " ['0FD65E8743D13BD1',\n",
       "  'classic_bike',\n",
       "  '2025-02-03 09:13:12.382',\n",
       "  '2025-02-03 09:20:09.584',\n",
       "  'Forsyth St & Grand St',\n",
       "  '5382.07',\n",
       "  'Spruce St & Nassau St',\n",
       "  '5137.10',\n",
       "  '40.71779817737835',\n",
       "  '-73.99316132068634',\n",
       "  '40.71146364',\n",
       "  '-74.00552427',\n",
       "  'member',\n",
       "  'no']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrair só os nomes das estações de início\n",
    "station_names_rdd = valid_rdd.map(lambda row: row[4])\n",
    "\n",
    "# Agrupar por nome\n",
    "grouped_rdd = station_names_rdd.groupBy(lambda name: name)\n",
    "\n",
    "# Contar ocorrências \n",
    "station_counts_rdd = grouped_rdd.map(lambda pair: (pair[0], len(list(pair[1]))))\n",
    "\n",
    "# Usar reduce para manter só top 3\n",
    "def top3_reducer(a, b):\n",
    "    # Garante que 'a' e 'b' sejam listas de tuplas\n",
    "    if not isinstance(a, list):\n",
    "        a = [a]\n",
    "    if not isinstance(b, list):\n",
    "        b = [b]\n",
    "    \n",
    "    # Combina os dois conjuntos de dados\n",
    "    combined = a + b\n",
    "    # Ordena e mantém as 3 mais populares\n",
    "    top_3_combined = sorted(combined, key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    return top_3_combined\n",
    "\n",
    "# Printar as 3 estações mais populares\n",
    "top3_stations = station_counts_rdd.reduce(top3_reducer)\n",
    "print(\"Top 3 stations (current step):\", top3_stations)\n",
    "\n",
    "# Broadcast\n",
    "top3_station_names = set([name for name, _ in top3_stations])\n",
    "broadcast_top3 = sc.broadcast(top3_station_names)\n",
    "\n",
    "# Map para adicionar a coluna de top3\n",
    "def append_top3_flag(row):\n",
    "    return row + [\"yes\" if row[4] in broadcast_top3.value else \"no\"]\n",
    "\n",
    "tagged_rdd = valid_rdd.map(append_top3_flag)\n",
    "\n",
    "tagged_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d81b6-a189-43ea-b1b5-e07d69376a3a",
   "metadata": {},
   "source": [
    "### Step 4: Use Accumulators for Data Statistics\n",
    "1. Generate:\n",
    "   - Total trips\n",
    "   - Trips with missing data\n",
    "   - Trips by casual riders vs. members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc19ad47-0552-4d85-a5a6-44b5332ecb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================>                        (7 + 5) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips: 2031257\n",
      "Invalid trips (with missing data): 5295\n",
      "Casual trips: 192759\n",
      "Member trips: 1838498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Accumulators for statistics\n",
    "total_trips = sc.accumulator(0)\n",
    "invalid_trips = sc.accumulator(0)\n",
    "casual_trips = sc.accumulator(0)\n",
    "member_trips = sc.accumulator(0)\n",
    "\n",
    "# Função para contar as viagens\n",
    "def count_trips(row):\n",
    "    total_trips.add(1)\n",
    "    \n",
    "    # Verificar se há dados faltando\n",
    "    if len(row) != 13 or any(not field for field in row):\n",
    "        invalid_trips.add(1)\n",
    "    \n",
    "    # Contabilizar as viagens por tipo de usuário\n",
    "    if row[12] == \"casual\":  \n",
    "        casual_trips.add(1)\n",
    "    elif row[12] == \"member\": \n",
    "        member_trips.add(1)\n",
    "\n",
    "# Aplicar a contagem nas viagens\n",
    "valid_rdd.foreach(count_trips)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Total trips: {total_trips.value}\")\n",
    "print(f\"Invalid trips (with missing data): {invalid_trips.value}\")\n",
    "print(f\"Casual trips: {casual_trips.value}\")\n",
    "print(f\"Member trips: {member_trips.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648dde2-5000-4a04-b1a9-147ff38501e5",
   "metadata": {},
   "source": [
    "### Step 5: Other Insights\n",
    "1. Average trip duration for members vs. casual riders.\n",
    "2. Peak riding hours, i.e., the day hour in which more people are riding bikes.\n",
    "\n",
    "Tip: use `datetime` to format string dates and calculate duration, among other date data manipulations. An example below:\n",
    "\n",
    "```\n",
    "start_str = '2025-02-06 16:54:02.517'\n",
    "end_str = '2025-02-06 17:00:48.166'\n",
    "start_time = datetime.strptime(cols[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = datetime.strptime(cols[3], \"%Y-%m-%d %H:%M:%S\")\n",
    "duration = (end_time - start_time).total_seconds() / 60  # Convert to minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6282d14-5691-4fa5-999c-3efb2df3aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================================>    (11 + 1) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração média das viagens de casuais: 15.10 minutos\n",
      "Duração média das viagens de membros: 9.64 minutos\n",
      "A hora de pico é 17:00, com 194468 viagens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_row(row):\n",
    "    try:\n",
    "        # Parse times\n",
    "        start_time = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        end_time = datetime.strptime(row[3], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # minutos\n",
    "        user_type = row[12]\n",
    "        ride_hour = start_time.hour\n",
    "        return (user_type, duration, ride_hour)\n",
    "    except:\n",
    "        return None  # Ignora linhas com erro\n",
    "\n",
    "# Extrair (user_type, duration, ride_hour)\n",
    "parsed_rdd = valid_rdd.map(parse_row).filter(lambda x: x is not None)\n",
    "\n",
    "# Calcular duração média por tipo de usuário\n",
    "# => (user_type, (duration, 1)) para somar duração e contar viagens\n",
    "durations_rdd = parsed_rdd.map(lambda x: (x[0], (x[1], 1)))  # (tipo, (duração, 1))\n",
    "sum_counts = durations_rdd.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "averages = sum_counts.mapValues(lambda x: x[0] / x[1]).collectAsMap()\n",
    "\n",
    "# Descobrir hora de pico\n",
    "# => Mapear (hour, 1) e contar\n",
    "hours_rdd = parsed_rdd.map(lambda x: (x[2], 1))  # (hora, 1)\n",
    "hour_counts = hours_rdd.reduceByKey(lambda a, b: a + b)\n",
    "peak_hour_data = hour_counts.takeOrdered(1, key=lambda x: -x[1])  # maior contagem\n",
    "\n",
    "average_casual = averages.get(\"casual\", 0)\n",
    "average_member = averages.get(\"member\", 0)\n",
    "print(f\"Duração média das viagens de casuais: {average_casual:.2f} minutos\")\n",
    "print(f\"Duração média das viagens de membros: {average_member:.2f} minutos\")\n",
    "\n",
    "if peak_hour_data:\n",
    "    peak_hour, count = peak_hour_data[0]\n",
    "    print(f\"A hora de pico é {peak_hour}:00, com {count} viagens.\")\n",
    "else:\n",
    "    print(\"Não há dados suficientes para determinar a hora de pico.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
